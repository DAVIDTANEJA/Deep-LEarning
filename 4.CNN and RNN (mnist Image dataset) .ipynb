{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handwritten image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import tensorflow and MNIST dataset under the Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# using mnist dataset , 28x28 images of hand-written digits 0-9  \n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()   # now we unpack the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The MNIST database contains 60,000 training images and 10,000 testing images.\n",
    "##### x_train and x_test : contain greyscale RGB codes (from 0 to 255) \n",
    "##### y_train and y_test : contains labels from 0 to 9 which represents which numbers they actually are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  visualize these numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "image_index = 782            # select any number up to 60,000\n",
    "print(y_train[image_index])   # prints the label of number \n",
    "\n",
    "plt.imshow(x_train[image_index], cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 60000 represents the number of images in the train dataset and (28, 28) represents the size of the image: 28 x 28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape and Normalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The dataset is in 3-dimension, to use the dataset in Keras API need of 4-dims numpy arrays.\n",
    "##### Also normalize the data by dividing by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the array to 4-dim\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train : 60000\n",
      "Number of images in x_test : 10000\n"
     ]
    }
   ],
   "source": [
    "# Normalizing by dividing it to the max RGB value 255\n",
    "\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train :', x_train.shape[0])\n",
    "print('Number of images in x_test :', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "\n",
    "# Creating a Sequential Model and adding the layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))     # input_shape = (28, 28, 1)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())            # Flattening the 2D arrays to 1D array for fully connected layers\n",
    "\n",
    "model.add(Dense(128, activation=tf.nn.relu))    # 1st Dense layer (Hidden layer)\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(10,activation=tf.nn.softmax))   # final Dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can take any number for the first Dense layer, But the final Dense layer must have 10 neurons bcoz we have 10 number classes (0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 28)        280       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 28)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4732)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               605824    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 607,394\n",
      "Trainable params: 607,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 133s 2ms/step - loss: 0.5844 - acc: 0.8231A: 7:50 - loss: 2.3034 - acc: - ETA: 5:22 - loss: 2.3034 - ac - ETA: 4:10 - loss: 2.3002 - acc: 0.11 - ETA: 4:01 - loss: 2.3008 - acc: 0.1 - ETA: 3:47 - loss: 2.3010 - acc: 0. - ETA: 3:34 - loss: 2.3020 - acc - ETA: 3:16 - loss: 2.3016 - acc: 0 - ETA: 3:07 - loss: 2.3014 - acc: 0.11 - ETA: 3:04 - loss: 2.3012 - acc: 0.118 - ETA: 3:03 - loss: 2.3013 - acc: 0.1 - ETA: 2:59 - loss: 2.3003 - acc:  - ETA: 2:52 - loss: 2.3007 - acc: 0.11 - ETA: 2:50 - loss: 2.3005 - acc: 0. - ETA: 2:47 - loss: 2.3001 - acc: 0.1 - ETA: 2:44 - loss: 2.2998 - acc: 0.11 - ETA: 2:42 - loss: 2.2994 - acc:  - ETA: 2:38 - loss: 2.2976 - acc:  - ETA: 2:36 - loss: 2.2953 - acc: 0. - ETA: 2:34 - loss: 2.2937 -  - ETA: 2:29 - loss: 2.2876 - acc: - ETA: 2:26 - loss: 2.2823 - acc: - ETA: 2:24 - loss: 2.2750 - acc: - ETA: 2:22 - loss: 2.2682 - acc: 0. - ETA: 2:21 - loss: 2.2628 - ac - ETA: 2:18 - loss: 2.2491 - acc: 0.21 - ETA: 2:18 - loss: 2.2453 - acc: - ETA: 2:16 - loss: 2.2338 - acc: 0.229 - ETA: 2:16 - loss: 2.2316 - acc: 0.232 - ETA: 2:16 - loss: 2.2287 - - ETA: 2:13 - loss: 2 - ETA: 2:10 - loss: 2.1447  - ETA: 2:02 - loss: 1.940 - ETA: 2:00 - loss: 1.8830 - acc: 0.405 - ETA: 2:00 - loss - ETA: 1:58 - loss: 1.7944 - acc: 0 - ETA: 1:57 - loss: 1.7758 - acc: 0.443 - ETA: 1:57 - loss: 1.7723 - acc: 0 - ETA: 1:57 - loss - ETA: 1:54 - loss:  - ETA: 1:52 - loss: 1.6210 - acc: 0.495 - ETA: 1:52 - loss: 1.6186 - acc: 0.4 - ETA: 1:52 - loss: 1.6109 - acc: 0 - ETA: 1:51 - loss: 1.5977 - acc: 0. - ETA: 1:51 - loss: 1.586 - ETA: 1:50 - loss: 1.5481 - acc: 0.5 - ETA: 1:49 - loss:  - ETA: 1:47 - loss: 1.4 - ETA: 1:46 - loss: 1.4501 - acc: 0 - ETA: 1:45 - loss - ETA: 1:43 - loss: 1.3928 - acc: 0.5 - ETA: 1:43 - loss: 1.3855  - ETA: 1:42 - loss: 1.3618 - acc: 0 - ETA: 1:42 - loss: 1.3514 - acc: 0.58 - ETA: 1:41 - loss: 1.3480 - acc: 0.58 - ETA: 1:41 - loss: 1.3445 - acc: - ETA: 1:38 - loss: 1.2597 - acc: 0.612 - ETA: 1:38 - loss: 1.2580 - acc: 0.61 - ETA: 1:38 - loss: 1.2551 - a - ETA: 1:37 - loss: 1.2400 - acc:  - ETA: 1:36 - loss: 1.2313 - acc:  - ETA: 1:36 - loss: 1.2225 - - ETA: 1:35 - loss: 1.204 - ETA: 1:34 - ETA: 1:31 - loss: 1.1464 - acc: - ETA: 1:31 - loss: 1.1390 - acc: 0. - ETA: 1:31 - ETA: 1:28  - ETA: 1:26 - loss - ETA: 1:25 - loss: 1.0512 - acc: 0. - ETA: 1:24 - loss: 1. - ETA: 1:23 - loss: 1.0297 - a - ETA: 1:22 - loss: 1.0207  - ETA: 1:21 - loss: 1.0101 - acc: 0.69 - ETA: 1:21 - loss: 1.0082 - acc: 0 - ETA: 1:21 - loss: 1.0050 - acc: 0. - ETA: 1:20 - loss: 1.00 - ETA: 1:16 - loss: 0.9582 - acc: - ETA: 1:16 - loss: 0 - ETA: 1:14 - loss: 0.9396 - acc: 0.712 - ETA: 1:14 - loss: 0.9 - ETA: 1:13 - loss: 0.9272 - acc: 0.7 - ETA: 1:13 - loss: 0.9255 - acc - ETA: 1:12 - loss: 0.91 - ETA: 1:11 - loss: 0.9093 - acc: - ETA: 1:11 - loss: 0.9051 - acc: 0 - ETA: 1:10 - loss: 0.9023 - acc: 0.724 - ETA: 1:10 - ETA: 1:05 - loss: 0.8609 - acc: - ETA: 1:05 - loss: 0.8575 - - ETA: 1:04 - loss: 0.8507 - ETA: 1:03 - loss: 0.8439 - acc: 0 - ETA: 1:03 - loss: 0.8415 - acc: 0.7 - ETA: 1:02 - loss: 0.8402 - acc: 0.744 - ETA: 1:02 - loss: 0.8396 - ETA: 1:01 - loss: 0.8333  - ETA: 1:00 - loss: 0.8267 - acc: 0. -  - ETA: 58s - loss: 0.8131 - acc: 0. - ETA:  - ETA: 57s - loss: 0.8058 - a - ETA: 57s - loss: 0.8038 - a - ETA: 56s - loss: 0.8017 - - ETA: 56s - loss: 0.7990 - E - ETA: 53s - loss:  - ETA: 49s - loss: 0.7639 - ETA: 49s - loss: 0.7613 - acc: 0.76 - ETA: 49s - loss: 0.7609 - acc: 0. - ETA: 49s - loss: 0.7601 - acc:  - ETA: 47s -  - ETA: 46s - loss: 0.7469 - - ETA: 46s - loss: 0.7451 - acc: 0. - ETA: 46s - loss: 0.7445 - acc: 0. - ETA: 45s - loss: 0.7437 - - ETA: 45s - loss: 0.7413 - acc: 0.77 - ETA - ETA: 42s  - ETA: 42s - loss: 0.7252 - acc:  - ETA: 41s - loss: 0.7243 - acc - ETA: 38s - loss: 0.71 - ETA: 38s - loss: 0. - ETA: 37s - loss: 0.7056 - acc - ETA: 37s - loss: 0.7045 - acc:  - ETA: 34s - loss: 0.6931 - acc: 0.78 - ETA: 34s - loss: 0.6929 - acc - ETA:  - - ETA: 32s - loss: 0.6816 - acc: 0.79 - - ETA:  - ETA: 25s - loss: 0.6578 - acc - ETA: 25s - loss: 0.6570 - acc: 0.80 - E - ETA: 24s - loss: 0.6530 - acc: 0.80 - ETA: 24 - ETA: 22s - loss: 0.6494 - acc: 0.80 - ETA: 22s - loss: 0.6491 - a - ETA: 22s - loss: 0.6479 - acc: 0.80 - ETA: 22s - loss: 0.6476 - a - ETA: 22s - loss: 0.64 - ETA: 21s - loss: 0.6446 - acc - ETA: 21s - loss: 0.6438 - acc: 0.80 - E - ETA: 19s - loss: 0.6396 - - ETA: 19s - loss: 0. - ETA: 18s - loss: 0.6359 - acc: 0.80 - ETA: 18s - loss: 0.6357  - ETA - ETA: - ETA: 5s - loss: 0.5 - ETA: 4s - loss: 0.5939 - acc: 0. - ETA: 3s - loss: 0.5931 - acc: 0.820 - ETA: 3s - loss: 0.5930 - acc: 0 - ETA: 3s - loss: 0.5922 - acc: 0 - ETA: 2s - loss: 0.5915 - acc: 0.8\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 131s 2ms/step - loss: 0.2163 - acc: 0.9363A: 2 - ETA: 2:21 - loss: - ETA: 2:15 - loss: 0.2375 - acc: 0 - ETA: 2:15 - loss: 0.2392 - acc:  - ETA: 2:13 - loss: 0.2443 - acc:  - ETA: 2:12 - loss: 0.256 - ETA: 2:11 - los - ETA: 2:08 - loss: 0.2642 -  - ETA: 2:06 - loss: 0.2625 - acc: 0.923 - ETA: 2:06 - loss: 0.2625 - acc: 0.923 - ETA:  - ETA: 2:02 - loss: 0.2574 - acc: 0.924 - ETA: 2:02 - loss: 0.2580 - acc: 0 - ETA: 2:01 - loss: 0.2572 - acc: 0.9 - ETA: 2:01 - loss: 0.2585  - ETA: 2:00 - loss: 0.2559 - acc: 0.92 - ETA: 1:59 - loss: 0.2547 - acc: 0 - ETA: 1:59 - loss: 0.2558 - acc: 0.92 - ETA: 1:59 - loss: 0.2542 - acc: 0.9 - ETA: 1:58 - loss: 0.2545 - acc: 0. - ETA: 1:58 - loss: 0.2538 - acc: - ETA: 1:57 - loss: 0.2542 - acc:  - ETA: 1:56 - loss: 0.2547 - acc: 0 - ETA: 1:56 - loss: 0.2538 -  - ETA: 1:54 - loss: 0.2554 - acc: - ETA: 1:53 - loss: 0.2593 - acc: 0. - ETA: 1:53 - loss: 0.2615 - acc: 0.92 - ETA: 1:53 - loss: 0.2612 - acc:  - ETA: 1:52 - loss: 0.2600 - acc:  - ETA: 1:51 - loss: 0.2585 - acc: 0.925 - ETA: 1:51 - loss: 0.2582 - acc: 0.925 - ETA: 1:51 - loss: 0.2583 - acc: 0.9 - ETA: 1:51 - loss: 0.2576 - acc: 0.9 - ETA: 1:50 - loss: 0.2581 - acc: 0 - ETA: 1:5 - ETA: 1:46  - - ETA: 1:41 - loss: 0.2504 - acc: 0.9 - ETA: 1:37 - loss: 0.2475 - acc: 0.928 - ETA: 1:37 - loss: 0.2472 - acc: 0 - ETA: 1:37 - loss: 0.2465 - acc: 0.928 - ETA: 1:37 - loss: 0.246 - ETA: 1:27 - loss: 0.2391 - acc: 0.9 - ETA: 1:27 - loss: 0.2392 - acc: 0.931 - ETA: 1:27 - loss: 0.23 - ETA: 1:26 - loss: 0.2388 - acc: 0.931 - ETA: 1:26 - loss: 0.2388 - acc: 0 - ETA: 1:25 - loss: 0.2392 - acc: 0.93 - ETA: 1:25 - loss: 0.2389 - acc: 0. - ETA: 1:25 - - ETA: 1:14 - loss: - ETA: 1: - ETA: 1:11 - los - ETA: 23s -  - ETA: 23s - loss:  - E - ETA:  - ETA: 18s - loss: 0. - ETA: 17s - loss: 0.2225 - acc:  - ETA: 17s - loss: 0.2224 - acc:  - ETA: 17s - loss: 0.2225 - acc:  - ETA: 17s - loss: 0.22 - ETA: 16s - loss: 0.2223 - acc: 0.93 - ETA: 16s - loss - ETA: 15s -  - E - ETA: 8s - loss: 0.2190 - acc: 0.9 - ETA: 8s - loss: 0.2190 - acc: 0. - ETA: 8s - loss: 0.2189 - acc: 0.935 - ETA: 8s  - ETA: 6s - loss: 0.2184 - acc: 0.935 - ETA: 6s - loss: 0.2184 - ETA: 5s -  - ETA: 0s - loss: 0.2168 -\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 131s 2ms/step - loss: 0.1576 - acc: 0.9528A: 2:32 - loss: 0.1185 - acc: 0.9 - ETA: 2:26 - loss: 0.1276 - acc: 0 - ETA: 2:21 - loss: 0.1419  - ETA: 2:15 - loss: 0.1528 - acc: 0.963 - ETA: 2:15 - loss: 0.1542 - acc: 0. - ETA: 2:13 - loss: 0.1653 - ac - ETA: 2:11 - loss: 0.1746 - acc:  - ETA: 2:09 - loss: 0.1685 - acc - ETA: - ETA: 2:03 - loss: 0.1741 - acc: 0.9 - ETA: 2:02 - loss: 0.1716 - acc: 0. - ETA: 2:02 - loss: 0.1724 - acc: 0.952 - ETA - ETA: 1:55 - loss: 0.1727  - ETA: 1:55 - loss: 0.1721 - acc:  - ETA: 1:51 - loss: 0.1755  - ETA: 1:50 - loss: 0.1749 - ET - ETA: 1:3 -  - ETA: 1:14 - loss: 0.1688 - acc: 0. - ETA: 1:14 - loss: 0.1687  - ETA: 1:13 - los - ETA: 1:11 - loss: 0.16 - ETA: 1:10 - loss: 0.1 - ETA: 1:06 - - ETA: 1:04 - loss: 0.1670 - acc:  - ETA: 1:03 - loss: 0.1669 - acc: 0.950 - ETA: 1:03 - loss: 0.1669 - a - ETA: 1: - ETA:  - ETA: 50s - loss: 0.16 - - ETA: 23s  - ETA: 21s - loss: 0.1624 - - ETA: 20s - loss: 0.1622 - acc: 0. - ETA: 20s - loss:  - ETA: 15s - loss: 0.1621 - - ETA: 15s - loss:  - ETA: 14s - loss: 0.1618 - - ETA: 12s -  - ETA:  - ETA: 8s - loss: 0.1591 - - ET - ETA: 5s - loss: 0.1584 -  - ETA: 4s - loss: 0.1581 - acc: 0.952 - ETA: 4s - loss: 0.1580 - acc: 0. - ETA: 3s - loss: 0.1580 - a - ETA: 3s - loss: 0.158 - ETA: 2s - loss: 0.1578  - ETA: 1s - loss: 0.1577 - acc: 0. - ETA: 0s - loss: 0.1577 - acc:  - ETA: 0s - loss: 0.1576 - acc:\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.1289 - acc: 0.9609  - ETA: 2:19 - loss: 0.1630 - acc: 0.94 - ETA: 2:16 - loss: 0.1567 - acc: 0. - ETA: 2:14 - loss: 0.1576 - acc: 0.947 - ETA: 2:13 - loss: 0.1640 - acc: - ETA: 2:09 - loss: 0.1658 - acc: - ETA: 2:08 - loss: 0.1467 - acc: 0.953 - ETA: 2:07 - loss: 0.1473 - acc:  - ETA: 2:05 - loss: 0.1458 - acc - ETA: 2:05 - loss: 0.1385 - acc: 0. - ETA: 2:04 - loss: 0.1407 - acc:  - ETA: 2:03 - loss: 0.1356 - acc: 0.957 - ETA: 2:03 - loss - ETA: 2:00 - loss:  - ETA: 1:58 - loss: 0.1283 -  - ETA: 1:57 - loss: 0.1328 - acc: - ETA: 1:56 - loss: 0.1309 - acc: 0.96 - ETA: 1:56 - lo - ETA: 1:54 - loss: 0.1351 - acc: 0.95 - ETA: 1:53 - loss: 0.1340 - acc: 0.958 - ETA: 1:53 - loss: 0.1365 - acc:  - ETA: 1:53 - loss: 0.1371 - acc: 0 - ETA: 1:53 - loss: 0.1367 - acc: 0.95 - ETA: 1:53 - loss: 0.1366  - ETA: 1:52 - loss: 0.1382 - - ETA: 1:51 - loss: 0.1359 - acc:  - ETA: 1:50 - loss: 0.1355 - acc - ETA: 1:50 - loss: 0.1361 - acc:  - ETA: 1:49 - loss: 0.1345 - acc: - ETA: 1:49 - loss: 0.1339 - acc: - ETA: 1:48 - loss: 0.1346 - acc: 0.958 - ETA: 1:48 - loss: 0.1 - ETA: 1:47 - loss: 0.1368  - ETA: 1:46 - loss: 0.1365 - acc: 0 - ETA: 1:46 - loss: 0.1375 - acc: 0 - ETA: 1:45 - loss: 0.1363 - acc:  - ETA: 1:45 - loss: 0.1398 - acc: 0.9 - ETA: 1:45 - loss: 0.1398 - acc: 0.9 - ETA: 1:45 - loss: 0.1397 - acc: 0. - ETA: 1:44 - loss: 0.1386 - acc: 0.958 - ETA: 1:44 - loss: 0.1 - ETA: 1:43 - loss: 0.14 - ETA: 1:42 - loss: 0.1424 - acc: 0.957 - ETA: 1:42 - loss: 0.1420 - acc: 0 - ETA: 1:42 - loss: 0.1407 - acc: 0.95 - ETA: 1:42 -  - ETA - ETA: 1:32 - loss - ETA: 1:31 - loss: 0.1358 - acc: 0.95 - ETA: 1:30 - loss: 0.1357 - acc: - ETA: 1:30 - loss: 0.13 - ETA: 1:29 - loss: 0.1360 - ac - ETA: 1:28 - loss: 0.1 - ETA: 1:27 - loss:  - ETA: 1:26 - loss: 0.1367 - acc: 0. - ETA: 1:26 - loss: 0.1377 - acc: 0.9 - ETA: 1:26 - loss: 0.1375 - acc: 0. - ETA: 1:25 - loss: 0.1376 - acc: 0.958 - ETA: 1:25 - loss: 0.1376 - acc: 0. - ETA: 1:25 - loss: 0.1371 - acc: 0. - ETA: 1:25 - loss: 0.1367 - acc: 0.959 - ETA: 1:25 - loss: 0.1366 - acc: 0 - ETA: 1:25 - loss: 0.1363 - acc: 0.95 - ETA: 1:24 - loss: 0.1362 - acc:  - ETA: 1:24 - loss: 0.1360 - acc: 0.95 - ETA: 1:24 - loss: 0.1360 - acc: 0.95 - ETA: 1:24 - loss: 0.1358 - acc: 0.95 - ETA: 1:24 - loss: 0.1357 - ETA: 1:23 - loss: 0.1355 - acc: 0.959 - ETA: 1:23 - loss: 0.1355 - acc: 0.9 - ETA: 1:22 - los - ETA: 1:21 - loss: 0.1357 - acc: - ETA: 1:21 - loss: 0.1359 - a - ETA: 1:20 - loss: 0. - ETA: 1:19 - loss: 0.1367 - ETA: 1:18 - loss: 0.1356 - ac - ETA: 1:17 - loss: 0.1351 - - ETA: 1:16 - loss: 0.1356 - - ETA: 1:16 - loss: 0.1351 - a - ETA: 1:15 - loss: 0.1350 - ETA: 1:14 - loss: 0.1344 - acc: 0. - ETA: 1:14 - loss: 0.1340 - acc: 0.9 - ETA: 1:14 - loss: 0.1341 -  - ETA: 1:13 - loss: 0.1340 - acc:  - ETA: 1:12 - loss: 0.1343 - - ETA: 1:12 - loss: 0.1339 - acc - ETA: 1:11 - loss: - ETA: 1:10 - loss: 0.1340 - acc - ETA: 1:09 - loss: 0.1339 - acc: 0.959 - ETA: 1:09 - loss: 0.1340 - acc: 0.9 - ETA: 1:09 - loss: 0.1341 - acc:  - ETA: 1:09 - loss: 0.1 - ETA: 1:07 - loss: 0.1330 - acc: 0.960 - ETA: 1:07 - loss: 0.1330 - acc: 0.960 - ETA: 1:07 - loss: 0.1329 - acc: 0.960 - ETA: 1:07 - loss: 0.1329 - acc: 0.960 - ETA: 1:07 - loss: 0.1328 - acc: - ETA: 1:07 - loss: 0.1329 - acc - ETA: 1:06 - loss:  - ETA: 1:05 - loss: 0.1324 - acc: 0.9 - ETA: 1:05 - loss: 0.1322 - acc: - ETA: 1:04 - loss: 0.1327 - acc: 0 - ETA: 1:04 - loss: 0.1327 - acc: 0 - ETA: 1:03 - loss: 0.1324 - acc: 0.9 - ETA: 1:03 - loss: 0.1323 - acc: 0.960 - ETA: 1:03 - loss: 0.1323 - acc: 0.960 - ETA: 1:03 - loss: 0.1323 - acc: 0.9 - ETA: 1:03 - loss: 0.1323 - acc: 0.960 - ETA: 1:03 - loss: 0.1322 - acc: 0 - ETA: 1:02 - loss: 0 - ETA: 1:01 - loss: 0.1318 - acc: 0.96 - ETA: 1:01 - loss: 0.1317 - acc: 0.960 - ETA: 1:01 - loss: 0.1317 - acc: 0 - ETA: 1:01 - loss: 0.1316 - acc: 0.960 - ETA: 1:01 - loss: 0.1316 - acc: 0. - ETA: 1:00 - loss: 0.13 - ETA: 58s  - ETA: 57s - loss: 0.1295 - acc: 0.96 - ETA: 57s - loss: 0.1294 - acc: 0.96 - ETA: 57s - loss: 0.1294 - a - E - ETA: 56s - lo - ETA: 55s - loss: 0.1284 - acc: 0.96 - ETA: 55s - loss: 0.1283 - acc: 0.96 - ETA: 55s - lo - ETA: 53s - loss: 0.1284 - - ETA: 41s - lo - ETA: 39s - loss: 0.1287 - acc: 0.96 - ETA: 38s - loss:  - ETA: 38s - loss:  - ETA: 37s - loss: 0. - - ETA: 35s - loss: 0.1297 - acc: 0. - ETA - ETA: 34s - loss: 0.1296 - acc: 0. - ETA: 34s - loss: 0.1297 - acc:  - ETA: 34s - loss: 0.1296 - acc: 0. - ETA: 34s - loss: 0.1296 - acc - ETA: 33s - loss: 0.1295 - acc: 0.96 - ETA: 33s -  - ETA: 32s - loss: 0.1299 - acc: 0.96 - ETA: 32s - loss: 0.13 - ETA: 32s - loss: 0.1299 - - - ETA: 30s - loss: 0.1300 - acc: 0.96 - ETA: 30s - loss: 0.1300 - acc: 0. - ETA: 30s - loss: 0.1302 - acc:  - ETA: 30s  - ETA: 29s - loss: 0. - ETA: 28s -  - ETA: 26s - loss: 0.1298 - acc - ETA: 24s - loss: 0.1304 - - ETA: 24s -  - ETA: 23s - loss: 0.1307 - - ETA: 23s - loss: 0.1307 - - ETA: 22s - loss: 0.1307 - acc: 0.96 - ETA: 22s - loss: 0.1308 - acc: 0.96 - ETA - ETA: 21s - loss: 0.1304 - acc - ETA: 21s - loss: 0.1305 - acc - ETA: 21s - loss: 0.1303 - acc: 0.96 - ETA: 20s  - ETA: 18s - loss: 0.1301 - ETA: 18s - loss: 0.1300 - acc:  - ETA: 17s - loss: 0.1299 - ETA: 17s - loss: 0.1300 - acc: 0.96 - ETA: 17s - loss: 0.1299 - acc: 0. - ETA: 17s - loss: 0.1298 - acc: 0. - ETA: 17s - loss - ETA: 16s - loss: 0.1297 - acc: 0.96 - ETA: 16s - loss: 0.12 - ETA: 15s - loss: 0.1293 - acc: 0.96 - ETA: 15s - loss: 0.1292 - - ETA: 15s - loss: 0.12 - ETA: 12s - loss: 0.1287 - acc:  - ETA: 11s - loss: 0.1287 - acc - ETA: 11s - loss: 0.1287 - a - ETA: 11s - loss: 0.1287 - acc: 0.96 - ETA: 11s - loss: 0.1287 - acc:  - ETA: 11s -  - ETA: 10s - loss: 0.1286 - acc: 0. - ETA: 10s - loss: 0.1 - ETA: 9s - loss: 0.1284 - acc: - ETA: 8s - loss: 0.1 - ETA: 7s - loss: 0.1285 - acc: 0 - ETA: 7s - loss: 0.1285  - ETA: 6 - ETA: 4s - loss: 0.1287 - acc: 0.961 - ETA: 4s - loss: 0.1287 - acc: 0. - ETA: 3s - loss: 0.1287 - ETA: 3s - loss: 0.1284  - ETA: 2s - loss: 0.1288 - acc: 0 - ETA: 1s - loss: 0.1287 - a - ETA: 1s - loss: 0.12 - ETA: 0s - loss: 0.1289 - acc: 0.960 - 126s 2ms/step - loss: 0.1290 - acc: 0.9609\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 127s 2ms/step - loss: 0.1105 - acc: 0.9670A: 2:13 - loss: 0.1083 - acc: 0 - ETA: 2:12 - loss: 0.1076 - acc: 0.964 - ETA: 2:12 - loss: 0.1060 - acc: 0.965 - ETA: 2:11 - loss: 0.1044 - acc: 0.9 - ETA: 2:11 - loss: 0.1013 - acc: 0.96 - ETA: 2:10 - loss: 0.1065 - acc: 0.967 - ETA: 2:10 - loss: 0.1057 - acc: 0.9 - ETA: 2:09 - loss: 0. - ETA: 2:05 - loss: 0.1040 - acc: 0.966 - ETA: 2:04 - loss: 0.1090 - acc:  - ETA: 2:03 - loss: 0.1086 - acc: 0.964 - ETA: 2:03 - loss: 0.1075 - ac - ETA: 2:02 - loss: 0.1033 - a - ETA: 2:02 - loss: 0.1050 - acc: 0.9 - ETA: 2:02 - loss: 0.1047 - acc: 0.9 - ETA: 2:01 - loss: 0.1080 - acc: 0.966 - ETA: 2:01 - loss: 0.1074 - acc:  - ETA: 2:00 - loss: 0.1101 - acc: 0.965 - ETA: 2:00 - loss: 0.1104 -  - ETA: 1:59 - loss: 0.1144 - acc: 0.96 - ETA: 1:59 - loss: 0.1146 - acc: 0.963 - ETA: 1:59 - loss: 0.114 - ETA: 1:57 - loss: 0.1161 - acc - ETA: 1:56 - loss: 0.1193 - acc: 0.96 - ETA: 1:56 - loss: 0.1182 -  - ETA: 1:55 - loss: 0.1191 - acc: 0.963 - ETA: 1:55 - loss: 0.1186 - acc: 0.96 - ETA: 1:55 - loss: 0.1200 - acc: 0. - ETA: 1:54 - loss: 0.1194 - acc: 0 - ETA: 1:54 - loss: 0 - ETA: 1:53 - loss: 0. - ETA: 1:52 - loss: 0.1183 - acc: 0.963 - ETA: 1:52 - loss: 0.1182 - ETA: 1:51 - loss: 0.1174 - acc: 0. - ETA: 1:51 - loss: 0.1169 - acc: 0.9 - ETA: 1:51 - loss: 0.1177 - acc: 0.963 - ETA: 1:51 - loss: 0.1184 - acc: 0. - ETA: 1:50 - loss: 0.1171 - acc: 0.963 - ETA: 1:50 - loss: 0.1190 - acc: 0 - ETA: 1:50 - loss: 0.1176 - acc - ETA: 1:50 - loss: 0.1166 - acc: 0.9 - ETA: 1:50 - loss: 0.1161  - ETA: 1:49 - loss: 0.1175 - acc: 0. - ETA: 1:49 - loss - ETA: 1:47 - lo - ETA: 1:45 - loss: 0.1167 - acc: 0.96 - ETA: 1:45 - loss: 0.1163 - acc: 0.964 - ETA: 1:45 - loss: 0.1165  - ETA: 1:44 - loss: 0.1164 - acc: 0.965 - ETA: 1:44 - loss: 0.1161 - acc - ETA: 1:43 - ETA: 1:42 - loss: 0.1174 - acc: 0.964 - ETA: 1:41 - loss: 0.1172 - acc: 0.964 - ETA: 1:41 - loss: 0.1171 - acc: 0 - ETA: 1:41 - loss - ETA: 1:40 - loss: 0. - ETA: 1:39 - loss: 0.1155 - acc - ETA: 1:38 - loss: 0.1160 - - ETA: 1:38 - loss: 0.1149 - acc: 0.9 - ETA: 1:38 - loss: 0.1150  - ETA: 1:37 - - ETA: 1:34 - loss: 0.1182 - acc: 0.965 - ETA: 1:34 - loss: 0.1180 - acc: 0.965 - ETA: 1:34 - loss: 0.1179 -  - ETA: 1:34 - loss: 0.1175 - acc: 0.965 - ETA: 1:33 - loss: 0.1174 - acc: 0.9 - ETA: 1:33 - loss: 0.1175 - acc: 0.965 - ETA: 1:33 - loss: 0.1173 - acc: 0.9 - ETA: 1:33 - loss: 0.1176 - acc:  - ETA: 1:32 - l - ETA: 1:31 - loss: 0.1170 - a - ETA: 1:30 - lo - ETA: 1:28 - loss: 0.1158 - acc: 0. - ETA: 1:28 - loss: 0.1155 - acc: 0.96 - ETA: 1:27 - loss: 0.1158 - acc:  - ETA: 1:27 - loss: 0.1156 - acc:  - ETA: 1:27 - loss: 0.1151 - - ETA: 1:26 - loss: 0.1151 - acc: 0.96 - ETA: 1:26 - loss: 0.1159 - - ETA: 1:25 - loss: 0 - ETA: 1:24 - loss: 0.1155 - acc:  - ETA: 1:24 - loss: 0.1150 - acc: 0.966 - E - ETA: 1:22 - loss: 0.1167 - a - ETA: 1:21 - loss: 0.1162 - a - ETA: 1:20 - loss: 0.1161 - acc: 0.966 - ETA: 1:20 - loss: 0.1160 - acc: 0 - ETA: 1:20 - loss: 0.1155 - acc: 0.966 - ETA: 1:20 - loss: 0.1154 - acc: - ETA: 1:19 - loss: 0.1160 - a - ETA: 1:19 - loss: 0.1159 - acc: 0.9 - ETA: 1:18 - loss: 0.1160 - acc: - ETA: 1:18 - loss: 0.1159 - a - ETA: 1:17 - loss: 0.1161 - acc: 0. - ETA: 1:17 - loss:  - ETA: 1:15 - loss: 0.1158 - acc: 0.966 - ETA: 1:15 - loss: 0.1159 - acc: 0.96 - ETA: 1:15 - loss: 0.1159 - acc: 0.966 - ETA: 1:15 - loss: 0.1162 - acc: 0.966 - ETA: 1:15 - loss: 0.1162 - acc: 0 - ETA: 1:15 - loss: 0.116 - ETA: 1:14 - loss: 0.1158 - acc - ETA: 1:13 - loss: 0.1156 - acc: 0.966 - ETA: 1:13 - loss: 0.1154 - acc: 0. - ETA: 1:13 - loss: 0.1160 - acc: 0.96 - ETA: 1:13 - loss: 0.1163 - acc: 0 - ETA: 1:13 - loss: 0.1168 - acc - ETA: 1:12 - loss: 0.1167 - acc: - ETA: 1:12 - loss: 0.1165  - ETA: 1:11 - loss: 0.1161 - acc: 0. - ETA: 1:10 - loss: 0.1160 - acc: 0.96 - ETA: 1:10 - loss: 0.1158 - acc: 0.96 - ETA: 1:10 - loss: 0.1156 - acc: 0. - ETA: 1:10 - loss: 0.1154 - acc: 0 - ETA: 1:10 - loss: 0.1155 - ETA: 1:09 - loss: 0.1147 - acc:  - ETA: 1:08 - loss: 0.1150  - ETA: 1:07 - loss: 0.1152 - acc: 0. - ETA: 1:07 - loss: 0.1149 - acc: 0. - ETA: 1:07 - loss: 0.1146 - ac - ETA: 1:06 - loss: 0.1141 - acc: - ETA: 1:05 - loss: 0.1138 - acc: 0.96 - ETA: 1:05 - loss: 0.1137 - acc: 0.96 - ETA: 1:05 - loss: 0.1137 - acc: 0. - ETA: 1:05 - loss: 0.1135 - acc: - ETA: 1:04 - loss: 0.1133 - acc: 0.967 - ETA: 1:04 - loss: 0.1132  - ETA: 1:03 - loss: 0.1130 -  - ETA: 1:03 - loss: 0.1126 - acc: 0 - ETA: 1:02 - l - ETA: 1:01 - loss: 0.1127 - ETA: 58s - loss: 0.1120 - acc:  - ETA: 58s - loss: 0.1120 - acc: 0.96 - ETA: 58s - loss: 0.1119 - acc: 0. - ETA: 58s  - ETA: 51s - loss: 0.1126 - acc - ETA: 51s - loss: 0.1126 - acc: 0.96 - E - ETA: 49s - loss: 0.11 - ETA: 49s - loss: 0.1123 - acc - ETA:  - ETA: 47s - loss: 0.1121 - - ETA: 47s - loss: 0.1120 - acc: 0. - ETA: 47s - loss: 0.1119 - acc: 0. - ETA: 47s -  - ETA: 46s -  - ETA: 45s - loss: 0. - ETA: 44s - loss: 0.1118 - a - ETA: 43s - loss: 0.1113 - - ETA: 42s - loss: 0.1115 - acc: 0. - ETA: 42s - loss: 0.1114 - acc: 0. - ETA: 42s - loss: 0.1116 - acc: 0.96 - ETA: 42s - loss: 0.1115 - acc: 0. - ETA: 42s - loss: 0.1115 - acc - ETA: 41s - loss: 0.1113 - acc: 0.96 - ETA: 41s - loss: 0.1114 - acc: 0. - ETA: 41s - loss: 0.1113 - acc: 0. - ETA: 41s - loss:  - ETA: 40s - loss: 0.1113 - acc:  - ETA: 40s - loss: 0.1113 - acc: 0.96 - - ETA: 39s  - ETA: 38s - loss: 0.1114 - ETA: 37s - loss: 0.1112 - acc: 0. - ETA: 37s - loss: 0.11 - ETA: 37s - loss: 0.1114 - acc:  - ETA: 36s - loss: 0.1114 - acc - ETA: 36s - loss: 0.1113 - acc: 0. - ETA: 36s - loss: 0.1112 - acc: 0.96 - ETA: 36s - loss: 0.1111 - acc - ETA: 36s - loss: 0.1112 - acc - ETA:  - ETA: 33s - loss: 0.1109 - - ETA - ETA: 32s - loss: 0.1116 - a - ETA: 31 - ETA: 30s - loss: 0.1119 - acc: 0.96 - ETA: 30s - loss: 0.1118 - a - ETA: 30s - loss: 0.1118 - ETA: 29s  - E - ETA: 26s - loss: 0.1124 - acc: 0.96 - ETA: 26s - loss:  - ETA: 25s - loss: 0.1121 - acc: 0. - ETA: 25s - loss: 0.1121 - acc: 0.96 - ETA: 25s - loss: 0.1120 - acc: 0. - ETA: 25s - loss: 0.1121 - ETA: 24s - loss: 0.1122 - - ETA: 24s - loss: 0.1122 - acc - ETA: 21s - loss: 0.1122 - acc: 0. - ETA: 21s - loss: 0.1122 - a - ETA: 20s - loss: 0.1121 - ETA: 19s -  - ETA: 18s - loss: 0.1121 - a - ETA: 15s - loss: 0.1118 - ETA: 13s - loss - ETA: 12s - loss: 0.1111 - acc:  - ETA: 12s - loss: 0.1111 - acc: 0. - ETA: 12s - loss: 0.1110 - acc: 0.96 - ETA: 12s - loss: 0.1110 - a - ETA: 11s - loss: 0.1109 - a - ETA: 11s - loss: 0.1108 - - ETA: 10s - loss: 0.1109 - acc: 0.96 - ETA: 10s - loss: 0.1109 - acc:  - ETA: 10s - loss: 0.1108 - a - ETA: 10s - loss: 0.1 - ETA: 9s - loss: 0.1111 - acc: 0.966 - ETA: 9s - loss: 0.1111 - acc: 0.96 - ETA: 9s - loss: 0.1112 - a - ETA: 5s - loss: 0.1102 - acc: - ETA: 5s - loss: 0.1103 - acc: 0.9 - ETA: 5s - loss: 0.1104 - acc - ETA: 4s - loss: 0.1103 - acc: 0.96 - ETA: 4s - loss: 0.1102 - acc: 0.9 - ETA: 4s - loss: 0.1104 - acc: 0.96 - ETA: 4s - loss: 0.1104 - - ETA: 3s - los - ETA: 1s - loss: 0.1105 - acc:  - ETA: 1s - loss: 0.1106 - acc:  - ETA: 1s - loss: 0.11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x88e116ee10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=x_train,y=y_train, epochs=5)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 12s 1ms/step\n",
      "\n",
      " validation loss is :  0.08774380932422354\n",
      "validation accuracy :  97.38%\n"
     ]
    }
   ],
   "source": [
    "# Now calculate the validation loss and validation accuracy\n",
    "\n",
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('\\n validation loss is : ', val_loss)\n",
    "print(f'validation accuracy : {val_acc : 0.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation shows : 97.38%  accuracy with basic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In many image classification cases (e.g. for autonomous cars), we cannot even tolerate 0.1% error since, as an analogy, it will cause 1 accident in 1000 cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x88e669ada0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 555       \n",
    "\n",
    "pred = model.predict(x_test[image_index].reshape(1,28,28,1))\n",
    "print(pred.argmax())\n",
    "\n",
    "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM   \n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()            # x_train, x_test : give them in tuple\n",
    "\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bulding RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 565s 9ms/sample - loss: 0.5887 - acc: 0.8102 - val_loss: 0.1369 - val_acc: 0.9597\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 547s 9ms/sample - loss: 0.1572 - acc: 0.9570 - val_loss: 0.0925 - val_acc: 0.9739\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 571s 10ms/sample - loss: 0.1079 - acc: 0.9709 - val_loss: 0.0669 - val_acc: 0.9795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x9b5674b828>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CuDNNLSTM: is fast & run on GPU, it use: tanh activation\n",
    "# use : CuDNNLSTM inplace of LSTM, and remove 'activation' function from layers\n",
    "\n",
    "model = Sequential()                      \n",
    "\n",
    "model.add(LSTM(128, input_shape=(x_train.shape[1:]), activation='relu', return_sequences=True)) \n",
    "# return_sequence : if going to next layer need it, if going to Dense layer then 'no need' of this\n",
    "model.add(Dropout(0.2))   # its 20%\n",
    "\n",
    "model.add(LSTM(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))   \n",
    "\n",
    "# Now compile\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-5)   # deacay makes steps bit smaller to learn better of learning rate\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=3, validation_data=(x_test, y_test))      # change epochs accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the mdoel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 22s 2ms/sample - loss: 0.0669 - acc: 0.979514s - loss: 0.0932 - - ETA: 13s - loss: 0.09 - ETA: 13s - loss: 0.09 - ETA: 12s - loss: 0.0968 - - ETA: 12s -  - ETA: 1 - ETA: 9s - loss: 0.0865  - ETA: 5s - loss: 0.0766 -  - ETA: 4s - loss - ETA: 2s - los - ETA: 1s - loss: 0.0676 \n"
     ]
    }
   ],
   "source": [
    "# calculate the validation loss and validation accuracy\n",
    "\n",
    "val_loss, val_acc = model.evaluate(x_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss :  0.0669349164525629\n",
      "validation accuracy :  97.95%\n"
     ]
    }
   ],
   "source": [
    "print('validation loss : ', val_loss)\n",
    "print(f'validation accuracy : {val_acc : 0.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
