{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deep Learning : \n",
    "is a collection of statistical ML techniques used to learn feature hierarchies based on the \n",
    "concept of Artificial Neaural Networks.                                                                                   \n",
    "        \n",
    "    Input layer -> Hidden layer 1 -> Hidden layer 2 (or upto n no. of hidden layers)-> Output layer.                    \n",
    "        \n",
    "Deep learning is set of ML algorithms where the model tries to learn high level abstractions from the data.               \n",
    "Deep learning skips the feature extraction part or tries to keep it minimal, the model itself extracts the feature.           \n",
    "\n",
    "Applications :                                                                                                                \n",
    "    Computer vision , Speech recognition , NLP , Audio recognition , Transaction, many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A deep learning framework is an interface, library or a tool which allows us to build \n",
    "deep learning models more easily and quickly, without getting into the details of underlying algorithms. \n",
    "They provide a clear and concise way for defining models using a collection of pre-built and optimized components.\n",
    "-Tensorflow                                                                                                                 \n",
    "-Keras                                                                                                                     \n",
    "-Pytorch , etc.                                                                                                           \n",
    "\n",
    "Deep Learning (Gradient Descent, CNN , RNN, LSTM, Keras, Autoencoder neural Networks, RBM, GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms related :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Neuron :\n",
    "Just like a neuron forms the basic element of our brain, When we get the information, \n",
    "we process it and then we generate an output. Similarly, in case of a neural network, \n",
    "a neuron receives an input, processes it and generates an output which is either sent to other \n",
    "neurons for further processing or it is the final output.\n",
    "    \n",
    "2.Weights : \n",
    "When input enters the neuron, it is multiplied by a weight. \n",
    "For example, if a neuron has two inputs, then each input will have has an associated weight assigned to it. \n",
    "We initialize the weights randomly and these weights are updated during the model training process. \n",
    "The neural network after training assigns a higher weight to the input it considers more important \n",
    "as compared to ones which are considered less important. \n",
    "Weight = 0 , denotes that the particular feature is insignificant\n",
    "W1 - weights , x -inputs ,  after passing through the node the input becomes=> a*W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"neural-network1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.Bias :\n",
    "In addition to the weights, another linear component is applied to the input, called as bias. \n",
    "The bias is basically added to change the range of the weight multiplied input.\n",
    "like : x1*W1 + bias.\n",
    "This is the final linear component of the input transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.Activation function : \n",
    "Once the linear component is applied to the input, a non-linear function is applied to it. \n",
    "By applying : 'activation function' to the linear combination. \n",
    "The activation function translates the input signals to output signals.\n",
    "The output after application of the activation function would look something\n",
    "like : f(a*W1+b) , where f() is the activation function.\n",
    "    \n",
    "We have “n” inputs given as X1 to Xn and corresponding weights 'Wk1 to Wkn'. \n",
    "We have a bias given as 'bk'.\n",
    "The weights are 1st multiplied to its corresponding input and are then added together along with the bias.\n",
    "\n",
    "Let this be called as u,    ' u=∑w*x+b '\n",
    "The activation function is applied to u i.e. f(u) and we receive the final output from the neuron as yk = f(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"neural-network2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commonly applied ' Activation functions ' are :  Sigmoid, ReLU and softmax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a) Sigmoid : \n",
    "One of the most common activation functions used is Sigmoid. \n",
    "The sigmoid transformation generates a more smooth range of values between 0 and 1.\n",
    "sigmoid(x) = 1/(1+e^-x)  or e^x / (e^x+1)        # -x is power of e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sigmoid-activation-function.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b) ReLU (Rectified Linear Units) :\n",
    "Instead of sigmoids, the recent networks prefer using 'ReLu' activation functions for the hidden layers.\n",
    "f(x) = max(x,0).\n",
    "\n",
    "-The benefit of using ReLU is that it has a constant derivative value for all inputs greater than 0.\n",
    "-The function returns 0 if it receives any negative input, but for any positive value x, it returns that value back.\n",
    "-The constant derivative value helps the network to train faster.\n",
    "-The output of the function is X when X>0 and 0 for X<=0. The function looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"relu.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c) Tanh function :\n",
    "The tanh(z) function is a rescaled version of the sigmoid, and its output range is [ −1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d) Softmax :\n",
    "Softmax activation functions are normally used in the output layer for classification problems.\n",
    "Its similar to the sigmoid function, with the only difference being that the outputs are normalized to sum up to 1.\n",
    "The sigmoid function would work in case we have a binary output, \n",
    "however in case we have a multiclass classification problem, softmax makes it really easy \n",
    "to assign values to each class which can be easily interpreted as probabilities. \n",
    "It’s very easy to see it in this way – Suppose you’re trying to identify a 6 which might also look a bit like 8.\n",
    "The function would assign values to each number as below. \n",
    "We can easily see that the highest probability is assigned to 6, with the next highest assigned to 8 and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"softmax.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Neural Network :\n",
    "Neural Networks form the backbone of deep learning. \n",
    "The goal of a neural network is to find an approximation of an unknown function. \n",
    "It is formed by interconnected neurons. These neurons have weights, and bias \n",
    "which is updated during the network training depending upon the error. \n",
    "The activation function puts a nonlinear transformation to the linear combination which then generates the output. \n",
    "The combinations of the activated neurons give the output.\n",
    "    \n",
    "6. Input/ Output/ Hidden Layer :\n",
    "Simply, the 'input layer' is the one which receives the input and is essentially the 1st layer of the network.\n",
    "The 'output layer' is the one which generates the output or is the final layer of the network.\n",
    "The processing layers are the 'hidden layers' within the network. \n",
    "These hidden layers are the ones which perform specific tasks on the incoming data and \n",
    "pass on the output generated by them to the next layer.\n",
    "The input and output layers are the ones visible to us, while are the intermediate layers are hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"neural-network3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7. MLP (Multi Layer perceptron) :\n",
    "A single neuron would not be able to perform highly complex tasks. \n",
    "Therefore, we use stacks of neurons to generate the desired outputs. \n",
    "In the simplest network we would have an input layer, a hidden layer and an output layer.\n",
    "Each layer has multiple neurons and all the neurons in each layer are connected to all the neurons in the next layer.\n",
    "These networks can also be called as fully connected networks.\n",
    "    \n",
    "\n",
    "8. Forward Propagation : \n",
    "Forward Propagation refers to the movement of the input through the hidden layers to the output layers. \n",
    "In forward propagation, the information travels in a single direction FORWARD. \n",
    "The input layer supplies the input to the hidden layers and then the output is generated. There is no backward movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Cost Function :\n",
    "When we build a network, the network tries to predict the output as close as possible to the actual value. \n",
    "We measure this accuracy of the network using the cost/loss function. \n",
    "The cost or loss function tries to penalize the network when it makes errors.\n",
    "Our objective while running the network is to increase our prediction accuracy and to reduce the error, \n",
    "hence minimizing the cost function. \n",
    "The most optimized output is the one with least value of the cost or loss function.\n",
    "\n",
    "If define the 'cost function' to be the 'mean squared error', it can be written as –\n",
    "\n",
    "-->  C= 1/m ∑(y – a)^2   # where 'm' -number of training inputs, 'a' - predicted value , 'y' - actual value.\n",
    "\n",
    "The learning process revolves around minimizing the cost.\n",
    "    \n",
    "\n",
    "10. Gradient Descent :\n",
    "Gradient descent is an optimization algorithm for minimizing the cost. \n",
    "To think of it intuitively, while climbing down a hill you should take small steps and \n",
    "walk down instead of just jumping down at once. \n",
    "Therefore, what we do is, if we start from a point x, we move down a little i.e. delta h, \n",
    "and update our position to x-delta h and we keep doing the same till we reach the bottom. \n",
    "Consider bottom to be the minimum cost point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11. Learning Rate :\n",
    "The learning rate is defined as the amount of minimization in the cost function in each iteration.\n",
    "In simple terms, the rate at which we descend towards the minima of the cost function is the learning rate.\n",
    "We should choose the learning rate very carefully since it should neither be very large \n",
    "that the optimal solution is missed and nor should be very low that it takes forever for the network to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"learning-rate.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12. Backpropagation :\n",
    "When we define a neural network, we assign random weights and bias values to our nodes. \n",
    "Once we have received the output for a single iteration, we can calculate the error of the network. \n",
    "This error is then fed back to the network along with the gradient of the cost function \n",
    "to update the weights of the network. \n",
    "These weights are then updated so that the errors in the subsequent iterations is reduced.\n",
    "This updating of weights using the gradient of the cost function is known as back-propagation.\n",
    "In back-propagation the movement of the network is backwards, the error along with the gradient \n",
    "flows back from the output layer through the hidden layers and the weights are updated.\n",
    "    \n",
    "    \n",
    "13. Batches :\n",
    "While training a neural network, instead of sending the entire input in one go, we divide in input into several chunks of       equal size randomly. Training the data on batches makes the model more generalized as compared to the model built when the     entire data set is fed to the network in one go.\n",
    "    \n",
    "    \n",
    "14. Epochs :\n",
    "An epoch is defined as a single training iteration of all batches in both forward and back propagation. \n",
    "This means 1 epoch is a single forward and backward pass of the entire input data.\n",
    "The number of epochs you would use to train your network can be chosen by you. \n",
    "It’s highly likely that more number of epochs would show higher accuracy of the network, \n",
    "however, it would also take longer for the network to converge.\n",
    "Also you must take care that if the number of epochs are too high, the network might be over-fit.\n",
    "    \n",
    "15) Dropout :\n",
    "Dropout is a regularization technique which prevents over-fitting of the network. \n",
    "As the name suggests, during training a certain number of neurons in the hidden layer is randomly dropped. \n",
    "This means that the training happens on several architectures of the neural network on different combinations of the neurons. \n",
    "You can think of drop out as an ensemble technique, where the output of multiple networks is then used to produce the final output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dropout.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "16. Batch Normalization :\n",
    "As a concept, batch normalization can be considered as a dam we have set as specific checkpoints in a river.\n",
    "This is done to ensure that distribution of data is the same as the next layer hoped to get.\n",
    "When we are training the neural network, the weights are changed after each step of gradient descent.\n",
    "This changes the how the shape of data is sent to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN (Convolutional Neural Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "17.Filters :\n",
    "A filter in a CNN is like a weight matrix with which we multiply a part of the \n",
    "input image to generate a convoluted output.\n",
    "Let's assume we have an image of size 28*28. \n",
    "We randomly assign a filter of size 3*3, which is then multiplied with different 3*3 sections of the image to form what is known as a convoluted output.\n",
    "The filter size is generally smaller than the original image size.\n",
    "The filter values are updated like weight values during backpropagation for cost minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"filter-cnn.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "18.CNN (Convolutional neural network) :\n",
    "CNN are basically applied on image data.\n",
    "Suppose we have an input of size (28*28*3), If we use a normal neural network, \n",
    "there would be 2352(28*28*3) parameters.\n",
    "And as the size of the image increases the number of parameters becomes very large.\n",
    "We \"convolve\" the images to reduce the number of parameters (as shown above in filter definition).\n",
    "As we slide the filter over the width and height of the input volume we will produce a 2-D activation map\n",
    "that gives the output of that filter at every position.\n",
    "We will stack these activation maps along the depth dimension and produce the output volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CNN.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "19.Pooling :\n",
    "It is common to periodically introduce pooling layers in between the convolution layers.\n",
    "This is basically done to reduce a number of parameters and prevent over-fitting.\n",
    "The most common type of pooling is a pooling layer of filter size(2,2) using the MAX operation.\n",
    "What it would do is, it would take the maximum of each 4*4 matrix of the original image. \n",
    "You can also pool using other operations like Average pooling, but max pooling has shown to work better in practice.\n",
    "'min pooling' - is based on picking up the minimum value from the selected region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pooling.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20.Padding :\n",
    "Padding refers to adding extra layer of zeros across the images \n",
    "so that the output image has the same size as the input. This is known as same padding.\n",
    "After the application of filters the convolved layer in the case of same padding has the size equal to the actual image.\n",
    "Valid padding refers to keeping the image as such an having all the pixels of the image which are actual or “valid”.\n",
    "In this case after the application of filters the size of the length and the width of the output keeps getting reduced at each convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"padding.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "21.Data Augmentation : \n",
    "Data Augmentation refers to the addition of new data derived from the given data, \n",
    "which might prove to be beneficial for prediction.\n",
    "Example : \n",
    "it might be easier to view the cat in a dark image if you brighten it, or for instance, \n",
    "a 9 in the digit recognition might be slightly tilted or rotated. \n",
    "In this case, rotation would solve the problem and increase the accuracy of our model.\n",
    "By rotating or brightening we’re improving the quality of our data. This is known as Data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data-augmentation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN (Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "22.Recurrent Neuron :\n",
    "A recurrent neuron is one in which the output of the neuron is sent back to it for t time stamps.\n",
    "If you look at the diagram the output is sent back as input 't' times. \n",
    "The unrolled neuron looks like t different neurons connected together.\n",
    "The basic advantage of this neuron is that it gives a more generalized output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rnn1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "23.RNN(Recurrent Neural Network) :\n",
    "RNN are used especially for sequential data where the previous output is used to predict the next one.\n",
    "In this case the networks have loops within them.\n",
    "The loops within the hidden neuron gives them the capability to store information about \n",
    "the previous words for some time to be able to predict the output.\n",
    "The output of the hidden layer is sent again to the hidden layer for 't' time stamps.\n",
    "The unfolded neuron looks like the above diagram.\n",
    "The output of the recurrent neuron goes to the next layer only after completing all the time stamps.\n",
    "The output sent is more generalized and the previous information is retained for a longer period.\n",
    "    \n",
    "The error is then back propagated according to the unfolded network to update the weights.\n",
    "This is known as Backpropagation through time(BPTT).\n",
    "    \n",
    "RNN : are a type of Artificial neural network, designed to recognize in sequence of data, \n",
    "such as text, genomes, handwriting, the spoken word, or numerical time series data emanating from sensors, \n",
    "stock markets and government agencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24.Vanishing Gradient Problem :\n",
    "Vanishing gradient problem arises in cases where the gradient of the activation function is very small.\n",
    "During back propagation when the weights are multiplied with these low gradients,\n",
    "they tend to become very small and “vanish” as they go further deep in the network.\n",
    "This makes the neural network to forget the long range dependency.\n",
    "This generally becomes a problem in cases of recurrent neural networks \n",
    "where long term dependencies are very important for the network to remember.\n",
    "This can be solved by using activation functions like ReLu which do not have small gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "25.Exploding Gradient Problem :\n",
    "This is the exact opposite of the vanishing gradient problem, \n",
    "where the gradient of the activation function is too large.\n",
    "During back propagation, it makes the weight of a particular node very high w.r.t. the others rendering them insignificant.\n",
    "This can be easily solved by clipping the gradient so that it doesn’t exceed a certain value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
